# Credit Card Customer Segmentation - YDY 팀 분석

## 📊 데이터셋에 대한 소개

### 데이터 규모
- **Train 데이터**: 2,400,000건 (240만)
- **Test 데이터**: 600,000건 (60만)

### 데이터 구성 (8개 범주)
1. **회원정보** (1.회원정보): customer 관련 정보
   - Train shape: (2,400,000, 78)
   - Test shape: (600,000, 77)
   
2. **신용정보** (2.신용정보): credit 관련 정보
   - Train/Test shape: (2,400,000/600,000, 42)
   
3. **승인매출정보** (3.승인매출정보): sales transaction 정보
   - Train/Test shape: (2,400,000/600,000, 406/523)
   
4. **청구입금정보** (4.청구입금정보): billing 관련 정보
   - Train/Test shape: (2,400,000/600,000, 46)
   
5. **잔액정보** (5.잔액정보): balance 관련 정보
   - Train/Test shape: (2,400,000/600,000, 82)
   
6. **채널정보** (6.채널정보): channel 관련 정보
   - Train/Test shape: (2,400,000/600,000, 105)
   
7. **마케팅정보** (7.마케팅정보): marketing 관련 정보
   - Train/Test shape: (2,400,000/600,000, 64)
   
8. **성과정보** (8.성과정보): performance 관련 정보
   - Train/Test shape: (2,400,000/600,000, 49)

### 타겟 변수
- 신용카드 고객 세분화 (Credit Card Customer Segmentation)
- 고객 ID 기준으로 최종 예측값 도출

---

## 🎯 프로젝트 개요

### 프로젝트 목표
대규모 신용카드 거래 데이터를 통합 분석하여 고객을 세분화하는 머신러닝 모델 개발

### 주요 특징
- **거대 데이터 처리**: 240만 건의 training data 처리
- **다중 데이터 소스 통합**: 8개의 분산된 데이터셋 통합
- **메모리 최적화**: int64 → int32, float64 → float32 변환으로 메모리 효율화
- **불균형 데이터 처리**: 클래스 가중치 및 오버샘플링 적용
- **앙상블 학습**: 3개의 부스팅 알고리즘 조합
- **GPU 가속**: 대규모 데이터 처리 시 GPU 활용

### 특별한 점들
1. **전처리 비중**: 전체 코드의 97%를 차지하는 철저한 전처리
2. **변수 최적화**: 400+개 변수 중 상위 300개 선별로 효율성 증대
3. **다양한 검증**: Public/Private 리더보드 점수를 통한 성능 비교

---

## 🛠️ 기술 스택

### 프로그래밍 언어
- Python 3.x

### 주요 라이브러리
- **데이터 처리**: pandas, numpy
- **머신러닝 모델**:
  - XGBoost (GPU 버전)
  - LightGBM (GPU 버전)
  - CatBoost
- **데이터 불균형 처리**: imbalanced-learn (oversampling)
- **시각화**: matplotlib, seaborn
- **데이터 분석**: scipy

### 하드웨어
- GPU 환경 (NVIDIA CUDA 지원)
- 메모리: 충분한 RAM (대규모 데이터셋 처리용)

---

## 📋 프로젝트 주요단계

전체 프로세스 구성:
```
[Import] → [데이터 준비] → [데이터 전처리] → [특성 엔지니어링] → [앙상블 모델링] → [최종 예측]
```

각 단계별 비중:
- **데이터 준비 & 전처리**: 97%
- **특성 엔지니어링 & 모델링 & 예측**: 3%

---

## 📁 파일구조

### 주요 코드 파일

#### 1. 2등 코드 (성능이 우수한 코드)
- `[Private_2nd] XGBoost + LightGBM + CatBoost (Soft Voting) 상위 300개.ipynb`
- `[Private_2nd] XGBoost + LightGBM + CatBoost (Soft Voting) 상위 350개 변경.ipynb`
- `[Private_2nd] XGBoost + LightGBM + CatBoost (Soft Voting) 상위 400개 변경.ipynb`
- `[Private_2nd] XGBoost + LightGBM + CatBoost (Soft Voting) 상위 450개 변경.ipynb`

#### 2. 팀 코드 (통합된 코드)
- `Main(3) - XGboost, LightGBM, CatBoost 상위 변수 300개.ipynb`
- `Main(6) - XGboost, LightGBM, CatBoost 상위 변수 400개.ipynb`
- `Main(8) - XGboost, LightGBM, CatBoost 상위 변수 450개.ipynb`

#### 3. 분석 및 비교 파일
- `변수개수별 성능비교.ipynb`: 상위 300/350/400/450개 변수에 따른 성능 비교
- `발표 자료/`: 프로젝트 발표 자료 모음

### 문서 파일
- `2등 코드리뷰.txt`: 2등 코드의 상세 분석 및 리뷰
- `YDY Read me`: 팀 소개 문서
- `발표대사 기초.txt`: 발표 기초 자료

---

## 📈 데이터 분석 내용

### 각 데이터셋별 분석

#### 1. 회원정보 (Customer)
- 결측치 처리
- 자료형 변환 (문자열 → 숫자)
- 상관관계 분석
- 변수별 데이터 분포 분석

#### 2. 신용정보 (Credit)
- 결측치 처리
- 자료형 변환
- 신용 관련 변수의 분포 분석

#### 3. 승인매출정보 (Sales) - 가장 복잡한 데이터셋
분석 항목:
- BOM (Best Of Month) 변수
- R12M, R6M, R3M (최근 12/6/3개월) 변수
- 카테고리별 이용액: 쇼핑, 교통, 여유, 납부
- R12M 할부건수/할부금액
- RP (Revolving Payment) 관련 변수
- 카드론 관련 변수
- 온라인/오프라인 이용: 개월수, 금액, 건수
- 페이 서비스별 이용 현황
- 결측치 처리 및 자료형 변환

#### 4. 청구입금정보 (Billing)
- 청구액 및 입금액 관련 변수 분석
- 결측치 처리

#### 5. 잔액정보 (Balance)
- 카드 잔액 및 한도 관련 분석

#### 6. 채널정보 (Channel)
- 채널별 이용 현황
- 문자열 인코딩

#### 7. 마케팅정보 (Marketing)
- 마케팅 캠페인 수신 여부
- 모두 0값이므로 최종 모델에서 제외

#### 8. 성과정보 (Performance)
- 목표 변수 및 성능 지표
- 결측치를 중앙값으로 대체

---

## 🔍 탐색적 데이터 분석 (EDA)

### 데이터 통합 방식
- **Primary Key**: 고객 ID
- **Join 방식**: concat 및 merge를 통한 마스터 데이터셋 생성
- **결과**: 단일 데이터프레임으로 모델 학습 가능

### 주요 발견사항
- 대규모 데이터셋에서의 메모리 관리 중요성
- 개별 데이터셋의 결측치 패턴이 상이함
- 변수 간 다양한 상관관계 존재

---

## 🧹 전처리 (Data Preprocessing)

### 전체 전처리 프로세스 (전체 코드의 97%)

#### 1. Train Data 전처리
각 데이터셋별 동일한 방법 적용:

**기본 전처리 단계:**
1. 결측치 처리
   - 중앙값(median) 대체
   - 결측치 비율 80% 이상인 변수는 제거
   
2. 자료형 변환
   - 불필요한 문자열을 숫자로 변환
   - 메모리 최적화: int64 → int32, float64 → float32
   
3. 데이터 분포 분석
   - 변수별 확인 및 검증

#### 2. Test Data 전처리
- Train과 동일한 방법 적용하여 데이터 일관성 유지

#### 3. 데이터 통합
```
개별 정제 → concat/merge → 마스터 데이터셋
```

#### 4. 메모리 최적화 코드 활용
- 저장 시 항상 최적화 진행
- 다운스트림 처리에서 메모리 효율성 향상

### 전처리 결과

**Train Data 최종 형태**
- 통합 데이터: (2,400,000, X)
- X는 전처리 후 남은 전체 변수 개수

**Test Data 최종 형태**
- 통합 데이터: (600,000, X)

---

## 🎨 특성 공학 (Feature Engineering)

### 1. 클래스 가중치 계산
```
목적: 불균형 데이터 문제 해결
방법: 각 클래스의 샘플 수를 역수로 하는 가중치 계산
적용: 모델 학습 시 샘플별 가중치 부여
```

### 2. 변수 선택 (Feature Selection)
```
단계:
1. 전체 feature로 XGBoost 초기 학습
2. Feature Importance 계산
3. 상위 300개 변수 추출 및 선별

목적: 
- 모델 복잡도 감소
- 연산 효율성 향상
- 오버피팅 방지
```

### 3. 오버샘플링
```
목적: 소수 클래스 데이터 보강
방법: imbalanced-learn의 오버샘플링 기법
효과: 클래스 분포 개선 및 학습 성능 향상
```

### 성능 비교 (상위 변수 개수별)

| 변수 개수 | Public 점수 | Private 점수 | 점수 편차 |
|----------|-----------|------------|---------|
| 300개   | 0.69      | 0.68       | 0.013   |
| 350개   | 0.684     | 0.677      | 0.007   |
| 400개   | 0.698     | 0.682      | 0.016   |
| **450개** | **0.695**   | **0.6944**   | **0.0006** |

**최적 결과**: 상위 450개 변수 사용 시 Public/Private 점수 차이 최소화

---

## 🤖 모델링 (Model Training)

### 모델 선택 이유 (3대 앙상블 알고리즘)

#### 1. **XGBoost** (eXtreme Gradient Boosting)
- 높은 성능과 안정성
- GPU 가속 지원
- Feature importance 제공

#### 2. **LightGBM** (Light Gradient Boosting Machine)
- 빠른 학습 속도
- 메모리 효율성
- GPU 지원 가능

#### 3. **CatBoost** (Categorical Boosting)
- 범주형 변수 자동 처리
- 과적합에 강함
- 빠른 예측 속도

### 학습 전략

#### 단계 1: 데이터 준비
- 상위 300개 변수로 필터링
- 클래스 가중치 계산 및 매핑

#### 단계 2: 각 모델 학습

**공통 설정:**
- 변수: 상위 300개
- 오버샘플링: 소수 클래스 증강
- 클래스 가중치: 불균형 해결
- GPU 활용: 가속 학습

**XGBoost 학습**
```
- GPU 드라이버 설정
- 전체 Train 데이터로 학습 (검증 없음)
```

**LightGBM 학습**
```
- GPU driver 설치
- 전체 Train 데이터로 학습 (검증 없음)
```

**CatBoost 학습**
```
- 자동 범주형 변수 처리
- 전체 Train 데이터로 학습 (검증 없음)
```

#### 단계 3: 다양성 확보 (Diversity)
- 서로 다른 특성을 가진 세 가지 모델 조합
- 과적합 방지 및 일반화 성능 향상

---

## 🎯 파인튜닝 (Hyperparameter Tuning)

### 각 모델별 최적화

#### 1. XGBoost 파이널 모델
- 300개 상위 변수 사용
- 오버샘플링 적용
- 클래스별 가중치 적용
- GPU 환경에서 학습

#### 2. LightGBM 파이널 모델
- 300개 상위 변수 사용
- 오버샘플링 적용
- 클래스별 가중치 적용
- GPU 환경에서 학습

#### 3. CatBoost 파이널 모델
- 300개 상위 변수 사용
- 오버샘플링 적용
- 클래스별 가중치 적용

### 변수 개수 최적화

**변수 개수별 성능 비교 결과:**
- 300개 변수: Public 0.69, Private 0.68
- 350개 변수: Public 0.684, Private 0.677
- 400개 변수: Public 0.698, Private 0.682
- **450개 변수**: Public 0.695, Private 0.6944 ⭐ **최고 성능**

**최적화 결론**: 상위 450개 변수에서 최고 성능 달성, Public/Private 편차 최소화

---

## 📊 모델 평가 (Model Evaluation)

### 평가 전략

#### 1. **Soft Voting 앙상블**
```
프로세스:
1. 각 모델의 예측 확률값(predict_proba) 획득
2. 3개 모델의 확률값 평균 계산
3. 평균값 기반 최종 클래스 결정

장점:
- 단일 모델보다 안정적인 성능
- 과신(overconfidence) 감소
- 일반화 성능 향상
```

#### 2. **ID별 Mode Aggregation**
```
상황: 한 고객(ID)에 대해 여러 기록 존재
처리 방법: 최빈값(Mode)을 활용한 고객별 통합
결과: 고객 단위의 최종 예측값 도출
```

#### 3. **성능 메트릭**

**Public 리더보드**
- 최고 점수: 0.698 (400개 변수)
- 안정적 점수: 0.695 (450개 변수)

**Private 리더보드**
- 최고 점수: 0.6944 (450개 변수) ⭐
- 최소 편차: 0.0006 (450개 변수)

### 최종 성능 분석

| 메트릭 | 결과 |
|-------|-----|
| 최종 선택 변수 | 상위 450개 |
| Public 점수 | 0.695 |
| Private 점수 | 0.6944 |
| 편차 | 0.0006 |
| 앙상블 방식 | Soft Voting |
| 모델 조합 | XGBoost + LightGBM + CatBoost |

---

## 💡 주요 인사이트 및 제안

### 우수한 점
1. **철저한 전처리**: 97%의 코드가 전처리에 할당될 정도의 세밀한 데이터 정제
2. **메모리 최적화**: 데이터셋 저장 시 항상 메모리 최적화 적용
3. **효율적인 변수 선택**: 400+ 변수 중 상위 300개 선별로 효율성 극대화
4. **불균형 데이터 처리**: 클래스 가중치 및 오버샘플링 병행
5. **강력한 앙상블 전략**: 3개 모델의 Soft Voting으로 안정적 성능 확보

### 개선 제안
1. **개별 전처리의 이점**: 전처리를 분리하면 메모리 부담 감소
2. **승인매출정보 처리 확대**: 2등 코드의 승인매출정보 전처리 방법을 다른 팀 코드에도 적용 권장
3. **변수 개수 최적화**: 450개 변수 사용으로 최고 성능 달성

---

## 📚 참고 자료

### 코드 리뷰
- `2등 코드리뷰.txt`: 상세한 단계별 분석 및 코드 구조 설명

### 성능 비교
- `변수개수별 성능비교.ipynb`: 300/350/400/450개 변수별 성능 시각화 및 분석

### 발표 자료
- `발표 자료/` 폴더: 프로젝트 발표 및 설명 자료

---

## 👥 팀 정보

**팀 구성**: YDY (3인)
- 담당: 신용카드 고객 세분화 모델 개발
- 특화: 대규모 데이터 처리 및 앙상블 모델링

---

**최종 업데이트**: 2026년 2월 13일
