프로젝트 발표 대본: "데이터의 산을 넘어, 모델의 숲으로"
제1발표자: 서론 및 프로젝트 개요 (5분)
주제: 우리가 마주한 데이터와 도전 과제

[인사 및 소개] 안녕하세요, 신용카드 등급 판별 머신러닝 프로젝트 발표를 맡게 된 [이름]입니다. 저희 팀은 카드사의 고객 정보를 바탕으로 고객 등급(Segment)을 분류하는 모델을 구축했습니다.

[데이터의 특성 및 규모] 이번 프로젝트의 핵심은 '규모'와 '복잡성'이었습니다. 회원정보부터 마케팅, 성과 정보까지 총 8개 카테고리의 데이터를 다루었으며, 월별 40만 행의 데이터를 6개월 치 병합한 결과, 최종적으로 240만 행과 약 850개의 피처를 가진 거대 데이터셋이 탄생했습니다.

[초기 문제 인식: 메모리의 벽] 시작부터 저희는 '메모리 부족'이라는 큰 벽에 부딪혔습니다. 단순히 데이터를 합치는 것만으로도 컴퓨터가 멈추는 상황이었죠. 저희는 이를 해결하기 위해 단순한 병합이 아닌 **'데이터 효율화'와 '모듈화'**라는 전략을 세웠습니다.

[함수 제작 및 자동화] 반복되는 작업을 줄이기 위해 make_df와 같은 자동화 함수를 제작했습니다. 파일 경로 설정의 사소한 실수까지 잡아내며 데이터를 안전하게 불러오는 뼈대를 구축했습니다. 또한, 2등 수상자의 코드를 리뷰하며 '카테고리별 우선 병합 후 전처리'라는 인사이트를 얻어 적용했습니다. 이제 저희가 이 거대한 데이터를 어떻게 요리했는지 구체적인 최적화 전략을 소개해 드리겠습니다.

제2발표자: 본론 1 - 메모리와의 전쟁, 최적화 전략 (5분)
주제: 한정된 자원으로 대용량 데이터 요리하기

[메모리 최적화 기법] 데이터가 너무 커서 물리적 한계가 명확했습니다. 저희의 첫 번째 전략은 **'데이터 다운캐스팅'**이었습니다. 기본 64비트 숫자 데이터를 32비트로 낮추어 메모리 사용량을 50% 이상 획기적으로 줄였습니다.

[하드웨어 자원 극대화] 소프트웨어적 방법 외에 하드웨어 자원도 쥐어짜냈습니다. SSD의 빈 공간을 가상 메모리로 할당하고, CUDA 기반의 GPU 가속을 활용했습니다. GPU는 CPU보다 개별 코어의 성능은 낮지만, 수천 개의 코어가 동시에 작동하여 대용량 금융 데이터의 단순 반복 계산에서 압도적인 속도를 보여주었습니다.

[협업 중 발생한 이슈] 이 과정에서 팀원 간 Pandas 버전 차이로 인해 데이터 병합이 안 되는 '삽질'도 겪었습니다. 이를 통해 환경 동기화의 중요성을 배웠고, 매일 텍스트 파일로 작업 내역을 기록하는 시스템을 만들었습니다.

[주의사항 공유] 한 가지 주의할 점은, 다운샘플링된 데이터가 학습 시 다시 64비트로 변환되며 메모리가 폭증할 수 있다는 점이었습니다. 이를 방지하기 위해 학습 직전까지 메모리 상태를 체크하는 로직을 설계하며 안정성을 확보했습니다.

제3발표자: 본론 2 - 피처 엔지니어링과 도메인의 한계 (5분)
주제: 수많은 컬럼 속에서 의미 있는 보석 찾기

[전처리의 디테일] 850개의 피처 중에는 노이즈도 많았습니다. 분산이 0인 데이터를 과감히 삭제하고, 금융 데이터의 특성을 고려해 결측치를 처리했습니다. 금융 데이터는 평균값이 이상치에 민감하므로 중앙값과 최빈값을 주로 사용했고, 어떤 결측치는 '정보 없음' 자체가 유의미한 정보이기에 '기타'로 분류했습니다.

[AI와 도메인 지식의 결합] 생소한 금융 변수들은 AI의 도움을 받아 해석하고 카테고리별로 재분류했습니다. 예를 들어, 거주지 정보를 '수도권 여부'로 이진화하거나, '10회 이상' 같은 구간형 문자열을 수치형으로 매핑하여 모델이 이해하기 쉽게 만들었습니다.

[핵심 파생 변수 제작] 특히 저희는 타겟값(A, B등급)이 매우 적은 불균형을 해결하기 위해 직접 파생 변수를 설계했습니다.

우량후보여부: A, B등급의 한도 금액이 최소 5만 이상이라는 점에 착안해 이진화했습니다.

실질여유한도: 한도는 높고 소진율은 낮은 A등급의 특징을 극대화한 변수입니다.

한도 대비 소비성향: 소진율에 로그를 씌워 한도 금액으로 나누어, 고등급 고객과 저등급 고객을 명확히 구분 짓는 점수를 부여했습니다.

이러한 파생 변수들은 모델이 희소한 A, B 등급을 찾는 강력한 나침반이 되었습니다.

제4발표자: 결론 - 모델링과 프로젝트 회고 (5분)
주제: 최적의 모델링과 우리가 얻은 교훈

[모델링 전략: 소프트 보팅] 저희의 최종 선택은 **XGBoost, LightGBM, CatBoost를 결합한 '소프트 보팅'**이었습니다. 특히 CatBoost는 카테고리형 변수가 많고 클래스 불균형이 심한 이번 데이터셋에서 가장 뛰어난 성능을 보였습니다.

[불균형 해소 전략] A, B 등급의 적은 데이터 수를 극복하기 위해 SMOTE 기법으로 데이터를 증량하고, 손실 함수에 **가중치(Weight)**를 적용했습니다. 이를 통해 모델이 단순히 다수결로 판정하지 않고, 소수 등급을 더 정밀하게 분류하도록 유도했습니다.

[시행착오와 복구] 프로젝트 막바지, 전처리 순서가 꼬여 코드가 멈추거나 깃허브 충돌로 코드가 날아가는 아찔한 순간도 있었습니다. 하지만 매일 작성한 '작업 로그'와 '코드 리뷰' 덕분에 빠르게 복구할 수 있었습니다. '저장의 생활화'와 '기록의 중요성'을 뼈저리게 느낀 순간이었습니다.

[마무리 한마디] 이번 프로젝트를 통해 데이터 분석은 단순한 기술 스택의 나열이 아님을 깨달았습니다. 데이터 규모에 맞는 인프라 최적화, 도메인을 이해하는 피처 엔지니어링, 그리고 팀원 간의 환경 동기화가 조화를 이룰 때 비로소 모델의 숲을 완주할 수 있었습니다. 이상으로 발표를 마치겠습니다. 감사합니다!